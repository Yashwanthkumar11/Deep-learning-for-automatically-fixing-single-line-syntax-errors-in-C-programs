{"nbformat":4,"nbformat_minor":5,"metadata":{"kernelspec":{"display_name":"Python 3.7 (tensorflow)","language":"python","name":"tf"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"name":"demo_eval.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uuy7KpZhz-V","executionInfo":{"status":"ok","timestamp":1620641488529,"user_tz":-330,"elapsed":27530,"user":{"displayName":"{ Yash }","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_zAdDe_l-p_OLxFdPQGG-RZPUPl3jR2KPzpuQBw=s64","userId":"12214982618796573874"}},"outputId":"0de62ea9-76ef-47cc-fee3-d66c24e7ece8"},"source":["\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)"],"id":"1uuy7KpZhz-V","execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RhP8Hj_ZiFO4","executionInfo":{"status":"ok","timestamp":1620641491906,"user_tz":-330,"elapsed":1148,"user":{"displayName":"{ Yash }","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_zAdDe_l-p_OLxFdPQGG-RZPUPl3jR2KPzpuQBw=s64","userId":"12214982618796573874"}},"outputId":"f3c9a40a-1932-4c8d-8cae-68cfc8f286be"},"source":["%cd /content/gdrive/MyDrive/nlp"],"id":"RhP8Hj_ZiFO4","execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/nlp\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"02cc20a4"},"source":["import sys\n","import ast\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from vocabulary import Vocabulary"],"id":"02cc20a4","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"61705acb"},"source":["class Evaluate():\n","    def __init__(self):\n","        self._model_path = 'model'\n","        self._load_model()\n","        self._input_vocabulary = Vocabulary('')\n","        self._input_vocabulary.load_data('input_vocabulary.pickle')\n","        self._target_vocabulary = Vocabulary('')\n","        self._target_vocabulary.load_data('target_vocabulary.pickle')\n","        self._load_configs()\n","\n","    def _load_configs(self):\n","        self.num_encoder_tokens = self._input_vocabulary.len_frequent_vocab()\n","        self.num_decoder_tokens = self._target_vocabulary.len_frequent_vocab()\n","        self.max_encoder_seq_length =  100 \n","        self.max_decoder_seq_length =  100\n","\n","        \n","\n","    def _load_model(self):\n","        self._model = keras.models.load_model(self._model_path)\n","        latent_dim = 256\n","        encoder_inputs = self._model.input[0]  # input_1\n","        encoder_outputs, state_h_enc, state_c_enc = self._model.layers[2].output  # lstm_1\n","        encoder_states = [state_h_enc, state_c_enc]\n","        self.encoder_model = keras.Model(encoder_inputs, encoder_states)\n","\n","        decoder_inputs = self._model.input[1]  # input_2\n","        decoder_state_input_h = keras.Input(shape=(latent_dim,), name=\"input_3\")\n","        decoder_state_input_c = keras.Input(shape=(latent_dim,), name=\"input_4_\")\n","        decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","        decoder_lstm = self._model.layers[3]\n","        decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n","            decoder_inputs, initial_state=decoder_states_inputs\n","        )\n","        decoder_states = [state_h_dec, state_c_dec]\n","        decoder_dense = self._model.layers[4]\n","        decoder_outputs = decoder_dense(decoder_outputs)\n","        self.decoder_model = keras.Model(\n","            [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n","\n","\n","    def _convert_to_list(self, tokens):\n","        tokens = [ast.literal_eval(src) for src in tokens]\n","        return list(tokens)\n","    \n","    def load_dataset(self, path):\n","        df  = pd.read_csv(path)\n","        sourceLineTokens = df['sourceLineTokens']\n","        input_texts = self._convert_to_list(sourceLineTokens)\n","\n","        encoder_input_data = np.zeros((len(input_texts), \n","                                    self.max_encoder_seq_length, self.num_encoder_tokens), \n","                                    dtype=\"float32\")\n","        for i, input_text in enumerate(input_texts):\n","            for t, char in enumerate(input_text[:self.max_encoder_seq_length]):\n","                encoder_input_data[i, t, self._input_vocabulary.to_index(char)] = 1.0\n","            \n","            if len(input_text) < self.max_encoder_seq_length:\n","                encoder_input_data[i, t + 1 :, self._input_vocabulary.PAD_token] = 1.0\n","        return (input_texts, encoder_input_data)\n","\n","    \n","    \n","    def decode_sequence(self, input_seq):\n","        # Encode the input as state vectors.\n","        states_value = self.encoder_model.predict(input_seq)\n","\n","        # Generate empty target sequence of length 1.\n","        target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n","        # Populate the first character of target sequence with the start character. target_token_index[\"\\t\"]\n","        target_seq[0, 0, Vocabulary.SOS_token] = 1.0\n","\n","        # Sampling loop for a batch of sequences\n","        # (to simplify, here we assume a batch of size 1).\n","        stop_condition = False\n","        decoded_sentence = []\n","        while not stop_condition:\n","            output_tokens, h, c = self.decoder_model.predict([target_seq] + states_value)\n","\n","            # Sample a token\n","            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","            sampled_char = self._target_vocabulary.to_token(sampled_token_index) #reverse_target_char_index[sampled_token_index]\n","            decoded_sentence.append(sampled_char)#decoded_sentence += sampled_char\n","\n","            # Exit condition: either hit max length\n","            # or find stop character. \"\\n\"\n","            if sampled_char == self._input_vocabulary.to_token(Vocabulary.EOS_token) or len(decoded_sentence) > self.max_decoder_seq_length:\n","                stop_condition = True\n","\n","            # Update the target sequence (of length 1).\n","            target_seq = np.zeros((1, 1, self.num_decoder_tokens))\n","            target_seq[0, 0, sampled_token_index] = 1.0\n","\n","            # Update states\n","            states_value = [h, c]\n","        decoded_sentence = list([token for token in decoded_sentence \n","                         if token != self._target_vocabulary.to_token(Vocabulary.PAD_token)])\n","        return decoded_sentence\n","    \n","    def evaluate(self, encoded_data):\n","        decoded_tokens = []\n","        \n","        for i in range(len(encoded_data)):\n","            try:\n","                input_seq = encoded_data[i :  i + 1]\n","                decoded_token = self.decode_sequence(input_seq)\n","                decoded_tokens.append(decoded_token)\n","            except Exception as e:\n","                print(f'[EXCEPTION] --> {e}')\n","        \n","        return decoded_tokens\n","\n","\n","\n","\n","\n"],"id":"61705acb","execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bb7cc979","executionInfo":{"status":"ok","timestamp":1620642052127,"user_tz":-330,"elapsed":538471,"user":{"displayName":"{ Yash }","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg_zAdDe_l-p_OLxFdPQGG-RZPUPl3jR2KPzpuQBw=s64","userId":"12214982618796573874"}},"outputId":"be3e2eac-06c9-481f-b1df-7db73be78fb2"},"source":["\n","input_csv = 'input-csv-file.csv'#sys.argv[1]\n","output_csv = 'output-csv-file.csv'#sys.argv[2]\n","evaluater = Evaluate()\n","texts, encoded = evaluater.load_dataset(input_csv)\n","print('Text Samples Found: ', len(texts))\n","print('encoded shape: ', encoded.shape)\n","print('Evaluation in progress - Please Wait')\n","decoded = evaluater.evaluate(encoded)\n","print('decoded length: ', len(decoded))\n","\n","texts = [str(src) for src in texts]\n","decoded = [str(src) for src in decoded]\n","output = {'sourceLineTokens':texts, 'targetLineTokens':decoded} \n","df = pd.DataFrame(output)\n","df.to_csv(output_csv, index=False)"],"id":"bb7cc979","execution_count":null,"outputs":[{"output_type":"stream","text":["Text Samples Found:  100\n","encoded shape:  (100, 100, 243)\n","Evaluation in progress - Please Wait\n","decoded length:  100\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3a94dd28"},"source":[""],"id":"3a94dd28","execution_count":null,"outputs":[]}]}